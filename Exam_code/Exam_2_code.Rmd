---
title: "7393 Exam 2 R Code Andrei Matveev"
output: html_notebook
---
```{r Data and set up}

library(data.table)
library(truncnorm)
library(coda)
devtools::install_github("stan-dev/loo")
library(loo)
library(rstan) 
library(shinystan)
library(invgamma)
library(bayesplot)
library(posterior)
library(rstanarm)


devtools::install_github("jfrench/bayesutils")

setwd('/Users/AM/Documents/_CU Masters/2020 fall Bayesian_7393/code/Bayesian_Statistics_Class_Code/Exam_code')
#df1 = load("diamonds_simple.rda")
#rm(df1, diamonds_simple)
df = bayesutils::diamonds_simple

df$lprice = log(df$price)
df$lcarat = log(df$carat)

df_covid = bayesutils::covid_dec4

```

```{r Problem 1 log density Function}

lq_theta_y = function(sigmaSQ, beta0, beta1, lpr = df$lprice, lcar = df$lcarat) {
  ld = dnorm(beta0, 0, 100, log = TRUE) + 
    dnorm(beta1, 0, 100, log = TRUE) + 
    dinvgamma(sigmaSQ, shape = 0.01, rate = 0.01, log = TRUE) +
    sum(dnorm(lpr, mean = (beta0 + beta1 * lcar), sd = sqrt(sigmaSQ), log = TRUE))
  return(ld)
}

```

```{r Problem 1 mh Function}

mh = function(B, theta_start) {
  theta = array(0, c((B+1), 3), dimnames = list(c(), c("sigmaSQ", "beta0", "beta1")))
  
  theta[1,1] = theta_start[1]
  theta[1,2] = theta_start[2]
  theta[1,3] = theta_start[3]

  for (i in 2:dim(theta)[1]) {
    
    ###  step for sigmaSQ
    beta0_star = theta[(i-1),2]
    beta1_star = theta[(i-1),3]
    
    sigmaSQ_star = rtruncnorm(n = 1, a=0, b=Inf, mean = theta[(i-1),1], sd = 0.1)

    num_logr = lq_theta_y(sigmaSQ = sigmaSQ_star, beta0 = beta0_star, beta1 = beta1_star) -
      log(dtruncnorm(x = sigmaSQ_star, a=0, b=Inf, mean = theta[(i-1),1], sd = 0.1))
    den_logr = lq_theta_y(sigmaSQ = theta[i-1, 1], beta0 = beta0_star, beta1 = beta1_star) -
      log(dtruncnorm(x = theta[i-1, 1], a=0, b=Inf, mean = sigmaSQ_star, sd = 0.1)) 
## !!! is the mean correct? Should  we use in den_logr mean = sigmaSQ_star or mean = theta[i-1, 1]? 
# check c-componentwise-mh v1 line 68, 72
    logr = num_logr - den_logr
    if (log(runif(1)) <= min(logr, 0)) {
      theta[i,1] = sigmaSQ_star
    } else {
      theta[i,1] = theta[(i - 1), 1]
    }
    
    ###  step for beta0
    beta1_star = theta[(i-1),3] # it is the repeated code, but I need it to keep the interpretability
    sigmaSQ_star = theta[i,1] # update sigmaSQ_star after the Gibbs step for sigmaSQ
    
    beta0_star = rnorm(1, theta[(i-1),2], 0.1) # !!! check the parametrization (0.1 or 0.1^2)
    
    num_logr = lq_theta_y(sigmaSQ = sigmaSQ_star, beta0 = beta0_star, beta1 = beta1_star) -
      dnorm(x = beta0_star, mean = theta[(i-1),2], sd = 0.1, log = TRUE)
    den_logr = lq_theta_y(sigmaSQ = sigmaSQ_star, beta0 = theta[i-1, 2], beta1 = beta1_star) -
      dnorm(x = theta[(i-1),2], mean = beta0_star, sd = 0.1, log = TRUE)
## !!! is the mean correct? Should  we use in den_logr mean = sigmaSQ_star or mean = theta[i-1, 1]? 
# check c-componentwise-mh v1 line 68, 72
    logr = num_logr - den_logr
    if (log(runif(1)) <= min(logr, 0)) {
      theta[i,2] = beta0_star
    } else {
      theta[i,2] = theta[(i - 1), 2]
    }
    
    ###  step for beta1
    sigmaSQ_star = theta[i,1] # it is the repeated code, but I need it to keep the interpretability
    beta0_star = theta[i,2] # update beta0_star after the Gibbs step for beta0

    beta1_star = rnorm(1, theta[(i-1),3], 0.1) # !!! check the parametrization (0.1 or 0.1^2)
    
    num_logr = lq_theta_y(sigmaSQ = sigmaSQ_star, beta0 = beta0_star, beta1 = beta1_star) -
      dnorm(x = beta1_star, mean = theta[(i-1),3], sd = 0.1, log = TRUE)
    den_logr = lq_theta_y(sigmaSQ = sigmaSQ_star, beta0 = beta0_star, beta1 = theta[i-1, 3]) -
      dnorm(x = theta[(i-1),3], mean = beta1_star, sd = 0.1, log = TRUE)
## !!! is the mean correct? Should  we use in den_logr mean = sigmaSQ_star or mean = theta[i-1, 1]? 
# check c-componentwise-mh v1 line 68, 72
    logr = num_logr - den_logr
    if (log(runif(1)) <= min(logr, 0)) {
      theta[i,3] = beta1_star
    } else {
      theta[i,3] = theta[(i - 1), 3]
    }
    
  }
  return(theta)
}

```

```{r Problem 1 mh Run}

B = 10^5 
keep = (B/2 + 1):(B + 1)
chain1 = mh(B, theta_start = c(0.1, -1, -1))
chain2 = mh(B, theta_start = c(0.3, 0, 0))
chain3 = mh(B, theta_start = c(0.5, -1, 1))
chain4 = mh(B, theta_start = c(0.2, 1, 1))

mc = mcmc.list(mcmc(chain1[keep,]), mcmc(chain2[keep,]),
                 mcmc(chain3[keep,]), mcmc(chain4[keep,]))
summary(mc)

```

```{r Problem 2 to assess the convergence}

keep = (B/2 + 19001):(B/2 + 20001)

mc = mcmc.list(mcmc(chain1[keep,]), mcmc(chain2[keep,]),
                 mcmc(chain3[keep,]), mcmc(chain4[keep,]))
coda::traceplot(mc)
coda::autocorr.plot(mc, lag.max = 100, auto.layout = TRUE)
gelman.diag(mc, autoburnin = FALSE)
geweke.diag(mc)

```

```{r Problem 3 Model A, include=FALSE}
stan_mod_A = "
data {
  int T;
  vector[T] lpr;    // log price 
  vector[T] lcar;   // log carat 
  real<lower=0> v;  // sample variance of log price 

}

parameters {
  real<lower=0> sigmaSQ;
  real beta0;
  real beta1;
}

transformed parameters {
  vector[T] mu;
  for (i in 1:T) {
    mu[i] = beta0 + beta1 * lcar[i];
  }

}

model {
  sigmaSQ ~ inv_gamma(0.01, 0.01);
  beta0 ~ normal(0, 100);
  beta1 ~ normal(0, 100);

  for (i in 1:T)
    lpr[i] ~ normal(mu[i], sqrt(sigmaSQ));
}
generated quantities {
  vector[T] log_lik;
  vector[T] y_rep;

  //vector[T] lik;
  real Rbsq;              // goodness-of-fit
  Rbsq = 1 - sigmaSQ/v;
  
  for (i in 1:T) {
    log_lik[i] = normal_lpdf(lpr[i] | mu[i], sqrt(sigmaSQ));
    y_rep[i] = normal_rng(mu[i], sqrt(sigmaSQ));
    //lik[i] = exp(log_lik[i]);
  }

}
"

T <- length(df$lprice)
v = var(df$lprice)
set.seed(90)

model_name = "Model_A_"
stan_dat_A = list(T = T, lpr = df$lprice, lcar = df$lcarat, v = v)
r_fit_A = stan(model_code = stan_mod_A, data = stan_dat_A, 
             iter = 5000, chains = 4, 
             control = list(max_treedepth = 21))

#file_name = paste(model_name, as.character(Sys.Date()), ".rda", sep="")
#setwd('/Users/AM/Documents/_CU Masters/2020 fall Bayesian_7393/code/Bayesian_Statistics_Class_Code/Exam_code')
#readRDS(r_fit, file = "") 
#saveRDS(r_fit, file = file_name, compress = "xz") 

summary(r_fit_A, pars = c("sigmaSQ", "beta0", "beta1"), prob = c(0.025, 0.975))$summary

#sso <- launch_shinystan(r_fit_A)

```

```{r Problem 3 Model B, include=FALSE}
stan_mod_B = "
data {
  int T;
  vector[T] pr;     // price 
  vector[T] car;    // carat 
  real<lower=0> v;  // sample variance of log price 

}

parameters {
  real<lower=0> sigmaSQ;
  real beta0;
  real beta1;
  real beta2;
}

transformed parameters {
  vector[T] mu;
  for (i in 1:T) {
    mu[i] = beta0 + beta1 * car[i] + beta2 * car[i] * car[i];
  }

}

model {
  sigmaSQ ~ inv_gamma(0.01, 0.01);
  beta0 ~ normal(0, 100);
  beta1 ~ normal(0, 100);
  beta2 ~ normal(0, 100);

  for (i in 1:T)
    pr[i] ~ normal(mu[i], sqrt(sigmaSQ));
}

generated quantities {
  vector[T] y_rep;
  vector[T] log_lik;
  //vector[T] lik;
  real Rbsq;              // goodness-of-fit
  Rbsq = 1 - sigmaSQ/v;
  
  for (i in 1:T) {
    log_lik[i] = normal_lpdf(pr[i] | mu[i], sqrt(sigmaSQ));
    y_rep[i] = normal_rng(mu[i], sqrt(sigmaSQ));

    //lik[i] = exp(log_lik[i]);
  }

}
"

T <- length(df$price)
v = var(df$price)
set.seed(91)

model_name = "Model_B_"
stan_dat_B = list(T = T, pr = df$price, car = df$carat, v = v)
r_fit_B = stan(model_code = stan_mod_B, data = stan_dat_B, 
             iter = 5000, chains = 4, 
             control = list(max_treedepth = 21))

#file_name = paste(model_name, as.character(Sys.time()), ".rda", sep="")
#setwd('/Users/AM/Documents/_CU Masters/2020 fall Bayesian_7393/code/Bayesian_Statistics_Class_Code/Exam_code')
#saveRDS(r_fit, file = file_name, compress = "xz") 
summary(r_fit_B, pars = c("sigmaSQ", "beta0", "beta1", "beta2"), prob = c(0.025, 0.975))$summary

sso <- launch_shinystan(r_fit_B)

```

```{r Problem 4 pp_check Model A}
# extracting the MCMC samples from the soda_fit object
samples = extract(r_fit_A)
ncycles = length(samples[[1]])
T <- length(df$price)

# each row of yrep is a sample from the pp distribution
yrep = matrix(0, ncol = T, nrow = ncycles)
for (i in seq_len(T)) {
  mui = samples$beta0 + samples$beta1 * df$lcarat[i]
  yrep[, i] = rnorm(ncycles, mean = mui, sd = sqrt(samples$sigmaSQ))
}

# approximate posterior predictive density for an observation
# with the same covariates as observation 1
#plot(density(yrep[,1]))
color_scheme_set("red")

# posterior predictive check
y = df$lprice
ppc_hist(y, yrep[sample(1:ncycles, 8),]) + ggtitle("Model A: histogram comparing the response to 8 replicated data sets")


# scatterplot of y vs average yrep
ppc_scatter_avg(y, yrep) + ggtitle("Model A: scatterplot comparing the response to the average replicated data set")

ppc_dens_overlay(y, yrep = yrep[sample(1:ncycles, 100),]) + ggtitle("Model A: density plot comparing the density of the response to the densities of 100 replicated data sets")

ppc_error_scatter_avg(y, yrep = yrep) + ggtitle("Model A: scatter plot comparing the response to the average predictive error")

ppc_intervals(y = y, yrep = yrep, x = df$lcarat) + ggtitle("Model A: Posterior predictive intervals to the observed data values") + xlab("log carat") + ylab("log price")
```

```{r Problem 4 pp_check Model B}
# extracting the MCMC samples from the soda_fit object
samples = extract(r_fit_B)
ncycles = length(samples[[1]])
T <- length(df$price)

# each row of yrep is a sample from the pp distribution
yrep = matrix(0, ncol = T, nrow = ncycles)
for (i in seq_len(T)) {
  mui = samples$beta0 + samples$beta1 * df$carat[i] + samples$beta2 * df$carat[i] * df$carat[i]
  yrep[, i] = rnorm(ncycles, mean = mui, sd = sqrt(samples$sigmaSQ))
}

# approximate posterior predictive density for an observation
# with the same covariates as observation 1
#plot(density(yrep[,1]))
color_scheme_set("green")

# posterior predictive check
y = df$price
ppc_hist(y, yrep[sample(1:ncycles, 8),]) + ggtitle("Model B: histogram comparing the response to 8 replicated data sets")


# scatterplot of y vs average yrep
ppc_scatter_avg(y, yrep) + ggtitle("Model B: scatterplot comparing the response to the average replicated data set")

ppc_dens_overlay(y, yrep = yrep[sample(1:ncycles, 100),]) + ggtitle("Model B: density plot comparing the density of the response to the densities of 100 replicated data sets")

ppc_error_scatter_avg(y, yrep = yrep) + ggtitle("Model B: scatter plot comparing the response to the average predictive error")

ppc_intervals(y = y, yrep = yrep, x = df$carat) + ggtitle("Model B: Posterior predictive intervals to the observed data values") + xlab("carat") + ylab("price")
```

```{r Problem 5 Model C, include=FALSE}
stan_mod_C = "
data {
  int T;
  vector[T] pr;    //  price 
  vector[T] lcar;   // log carat 
  real<lower=0> v;  // sample variance of log price 

}

parameters {
  real<lower=0> sigmaSQ;
  real beta0;
  real beta1;
}

transformed parameters {
  vector[T] mu;
  for (i in 1:T) {
    mu[i] = beta0 + beta1 * lcar[i];
  }

}

model {
  sigmaSQ ~ inv_gamma(0.01, 0.01);
  beta0 ~ normal(0, 100);
  beta1 ~ normal(0, 100);

  for (i in 1:T)
    pr[i] ~ lognormal(mu[i], sqrt(sigmaSQ));
}
generated quantities {
  vector[T] log_lik;
  vector[T] y_rep;

  //vector[T] lik;
  real Rbsq;              // goodness-of-fit
  Rbsq = 1 - sigmaSQ/v;
  
  for (i in 1:T) {
    log_lik[i] = lognormal_lpdf(pr[i] | mu[i], sqrt(sigmaSQ));
    y_rep[i] = lognormal_rng(mu[i], sqrt(sigmaSQ));
    //lik[i] = exp(log_lik[i]);
  }

}
"

T <- length(df$price)
v = var(df$price)
set.seed(92)

model_name = "Model_C_"
stan_dat_C = list(T = T, pr = df$price, lcar = df$lcarat, v = v)
r_fit_C = stan(model_code = stan_mod_C, data = stan_dat_C, 
             iter = 5000, chains = 4, 
             control = list(max_treedepth = 21))

#file_name = paste(model_name, as.character(Sys.Date()), ".rda", sep="")
#setwd('/Users/AM/Documents/_CU Masters/2020 fall Bayesian_7393/code/Bayesian_Statistics_Class_Code/Exam_code')
#readRDS(r_fit, file = "") 
#saveRDS(r_fit, file = file_name, compress = "xz") 

summary(r_fit_C, pars = c("sigmaSQ", "beta0", "beta1"), prob = c(0.025, 0.975))$summary

#sso <- launch_shinystan(r_fit_A)

```

```{r Problem 5 Model D, include=FALSE}
stan_mod_D = "
data {
  int T;
  vector[T] pr;     //  price 
  vector[T] lcar;   // log carat 
  vector[T] notic;  //  noticeable 

  real<lower=0> v;  // sample variance of log price 

}

parameters {
  real<lower=0> sigmaSQ;
  real beta0;
  real beta1;
  real beta2;
}

transformed parameters {
  vector[T] mu;
  for (i in 1:T) {
    mu[i] = beta0 + beta1 * lcar[i] + beta2 * notic[i];
  }

}

model {
  sigmaSQ ~ inv_gamma(0.01, 0.01);
  beta0 ~ normal(0, 100);
  beta1 ~ normal(0, 100);
  beta2 ~ normal(0, 100);


  for (i in 1:T)
    pr[i] ~ lognormal(mu[i], sqrt(sigmaSQ));
}
generated quantities {
  vector[T] log_lik;
  vector[T] y_rep;

  //vector[T] lik;
  real Rbsq;              // goodness-of-fit
  Rbsq = 1 - sigmaSQ/v;
  
  for (i in 1:T) {
    log_lik[i] = lognormal_lpdf(pr[i] | mu[i], sqrt(sigmaSQ));
    y_rep[i] = lognormal_rng(mu[i], sqrt(sigmaSQ));
    //lik[i] = exp(log_lik[i]);
  }

}
"

T <- length(df$price)
v = var(df$price)
set.seed(93)

model_name = "Model_D_"
stan_dat_D = list(T = T, pr = df$price, lcar = df$lcarat, 
                  notic = as.integer(df$inclusions == "noticeable") , v = v)
r_fit_D = stan(model_code = stan_mod_D, data = stan_dat_D, 
             iter = 5000, chains = 4, 
             control = list(max_treedepth = 21))

#file_name = paste(model_name, as.character(Sys.Date()), ".rda", sep="")
#setwd('/Users/AM/Documents/_CU Masters/2020 fall Bayesian_7393/code/Bayesian_Statistics_Class_Code/Exam_code')
#readRDS(r_fit, file = "") 
#saveRDS(r_fit, file = file_name, compress = "xz") 

summary(r_fit_D, pars = c("sigmaSQ", "beta0", "beta1", "beta2"), prob = c(0.025, 0.975))$summary

#sso <- launch_shinystan(r_fit_A)

```

```{r Problem 6a WAIC/LOOIC, include=FALSE}

waic_loo = function (fit) {
  log_lik = extract_log_lik(fit, merge_chains = FALSE)
  r_eff = exp(relative_eff(log_lik))
  wl = c(waic(log_lik)$waic, loo(log_lik, r_eff = r_eff)$looic)
  return(wl)
}
wl = array(0, dim=c(2,3), 
           dimnames = list(c("WAIC", "LOOIC"), 
                           c("Model B","Model C","Model D")))
wl[1,1] = waic_loo(r_fit_B)[1]
wl[2,1] = waic_loo(r_fit_B)[2]
wl[1,2] = waic_loo(r_fit_C)[1]
wl[2,2] = waic_loo(r_fit_C)[2]
wl[1,3] = waic_loo(r_fit_D)[1]
wl[2,3] = waic_loo(r_fit_D)[2]

wl
```

```{r Problem 6b effects plots}
samples = extract(r_fit_D)
ncycles = length(samples[[1]])
T <- length(df$price)
df$notic = as.integer(df$inclusions == "noticeable")

# each row of yrep is a sample from the pp distribution
yrep = matrix(0, ncol = T, nrow = ncycles)
for (i in seq_len(T)) {
  mui = samples$beta0 + samples$beta1 * df$lcarat[i] + samples$beta2 * df$notic[i]
  yrep[, i] = rlnorm(ncycles, mean = mui, sd = sqrt(samples$sigmaSQ))
}

color_scheme_set("blue")
ppc_intervals_grouped(y = df$price, yrep = yrep, x = df$carat, prob = 0.5, prob_outer = .95, group = df$notic) + ggtitle("Model D: Posterior predictive intervals to the observed data values") + xlab("lcarat") + ylab("price")


find_ep = function(yrep, min = 0.1, max = 2.4, step=0.1) {
  ep = array(0, dim=c((max-min+step)/step, 5), dimnames = list(c(), c("carat_min", "carat_max", "2.5%","Mean","97.5%")))
  n=0
  for (i in seq(min+step, max, by=step)) {
    n=n+1
    subset = ((df$carat>(i-step)) & (df$carat<=i))
    yrep_subset = yrep[,subset]
    ep[n,1] = i-step
    ep[n,2] = i
    q = quantile(yrep_subset, probs = c(0.025, 0.975))
    ep[n,3] = q[[1]]
    ep[n,4] = mean(yrep_subset)
    ep[n,5] = q[[2]]
  }
return(ep)
}

find_ep(yrep)
temp = ppc_intervals_data(y = df$price, yrep = yrep, x = df$lcarat, prob = 0.5, prob_outer = .95, group = df$notic)

```

```{r Problem 7_a Model E, include=FALSE}

stan_mod_E = "
data {
  int T;
  int deaths[T]; // COVID-19 deaths
  vector[T] log_pop;    // log of U.S. states population,
  vector[T] income; // median income (USD)
  vector[T] bs;     // percentage of the population with bachelor’s degrees.
}

parameters {
  real beta0;
  real beta1;
  real beta2;
}
transformed parameters {
  real log_lambda[T];
  for (i in 1:T) {
   log_lambda[i] = log_pop[i] + beta0 + beta1 * income[i] + beta2 * bs[i];
  }

}

model {
  beta0 ~ normal(0, 5);
  beta1 ~ normal(0, 5);
  beta2 ~ normal(0, 5);
 
  for (i in 1:T) {
    deaths[i] ~ poisson_log(log_lambda[i]);
  }
}
generated quantities {
  vector[T] log_lik;

  for (i in 1:T) {
    log_lik[i] = poisson_lpmf(deaths[i] | exp(log_lambda[i]));
  }  
  
}

"

T <- length(df_covid$bs)
set.seed(94)

model_name = "Model_E_"
stan_dat_E = list(T = T, deaths = df_covid$deaths, log_pop = log(df_covid$population), 
                  income = df_covid$income, bs = df_covid$bs)
r_fit_E = stan(model_code = stan_mod_E, data = stan_dat_E, 
             iter = 50000, chains = 2, 
             control = list(max_treedepth = 21))

summary(r_fit_E, pars = c("beta0", "beta1", "beta2"), prob = c(0.025, 0.975))$summary

#sso <- launch_shinystan(r_fit_E)
file_name = paste(model_name, as.character(Sys.Date()), ".rda", sep="")
setwd('/Users/AM/Documents/_CU Masters/2020 fall Bayesian_7393/code/Bayesian_Statistics_Class_Code/Exam_code')
#readRDS(r_fit, file = "") 
#saveRDS(r_fit_E, file = file_name, compress = "xz") 
```

```{r Problem 7_b rstanarm Model E, include=FALSE}
df_covid_rstan = data.frame(int = 1, 
                            deaths = df_covid$deaths,
                            log_pop = log(df_covid$population), 
                            income = df_covid$income,
                            bs = df_covid$bs)
fit_rstanarm_E = stan_glm(deaths ~ int + income + bs, 
                 offset = log_pop,
                 family = poisson(link = "log"),
                 data = df_covid_rstan,
                 iter = 10000, chains = 2)

summary(fit_rstanarm_E, digits = 6)

-0.000054 * 30697
(24191 - 39682) * -0.000054

(17.50 - 39.00) * 0.060348
```

```{r Problem 7_c STAN Model E WAIC LOOIC}
cat("STAN Model E WAIC", waic_loo(r_fit_E)[1])
cat("STAN Model E LOOIC", waic_loo(r_fit_E)[2])
```

```{r Problem 7_d STAN Model E ppc}
samples = extract(r_fit_E)
ncycles = length(samples[[1]])
T <- length(df_covid$bs)

# each row of yrep is a sample from the pp distribution
yrep = matrix(0, ncol = T, nrow = ncycles)
for (i in seq_len(T)) {
  log_l = log(df_covid$population[i]) +
    samples$beta0 + 
    samples$beta1 * df_covid$income[i] + 
    samples$beta2 * df_covid$bs[i]
  yrep[, i] = rpois(ncycles, lambda = exp(log_l))
}

ppc_intervals(y = y, yrep = yrep, x = df_covid$state_abb) + ggtitle("Model A: Posterior predictive intervals to the observed data values") + xlab("State") + ylab("Covid deaths")

```


